[
    "\\begin{table}[h!] \\begin{center} \\begin{footnotesize} \\begin{tabular}{l|ccc} \\hline {\\bf Model} & {\\bf CNN-A} & {\\bf CNN-B} \\\\ \\hline \\multirow{3}{*}{\\bf Architecture} & \\textsc{CONV 16 4×4+2} & \\textsc{CONV 32 5×5+2} \\\\ & \\textsc{ CONV 32 4×4+1 } & \\textsc{CONV 128 4×4+2} \\\\ & \\textsc{FC 100} & \\textsc{FC 250} \\\\ & \\textsc{FC 10} & \\textsc{FC 10} \\\\ \\hline \\end{tabular} \\end{footnotesize} \\end{center} \\caption{Architecture of CNN models used on MNIST and CIFAR-10. Each layer (except the last fully connected layer) is followed by ReLU activations. \\textsc{CONV t w×h+s} corresponds to a convolutional layer with \\textsc{t} filters of size \\textsc{w×h} with stride of \\textsc{s} in both dimensions. \\textsc{FC t} corresponds to a fully connected layer with \\textsc{t} output neurons. }\\label{tab:model_details} \\end{table}",
    "\\begin{table}[h!] \\begin{center} \\begin{footnotesize} \\resizebox{\\textwidth}{!}{% \\begin{tabular}{llll|cc|cc} \\hline & & & {\\bf Training} & \\multicolumn{2}{c|}{Accuracy} & \\multicolumn{2}{c}{Verified Accuracy} \\\\ {\\bf Dataset} & {\\bf Epsilon} & {\\bf Model} & {\\bf Epsilon} & {\\bf Nominal} & {\\bf PGD} & {\\bf SDP-FO (Ours)} & {\\bf LP} \\\\ \\hline \\multirow{1}{*}{\\mnist} & \\multirow{1}{*}{$\\epsilon = 0.3$} & CNN-A-Adv & $\\epsilon_\\mathrm{train} = 0.3$ & 98.6\\% & 80.0\\% & 43.4\\% & \\hp0.2\\%\\\\[1pt] \\hline \\multirow{4}{*}{\\cifar} & \\multirow{2}{*}{$\\epsilon = \\frac{2}{255}$} & CNN-A-Adv & $\\epsilon_\\mathrm{train} = \\frac{2.2}{255}$ & 68.7\\% & 53.8\\% & 39.6\\% & \\hp5.8\\% \\\\[1pt] & & CNN-A-Adv-4 & $\\epsilon_\\mathrm{train} = \\frac{4.4}{255}$ & 56.4\\% & 49.4\\% & 40.0\\% & 38.9\\%\\\\ [1pt] & \\multirow{2}{*}{$\\epsilon = \\frac{8}{255}$} & CNN-A-Adv & $\\epsilon_\\mathrm{train} = \\frac{8.8}{255}$ & 46.9\\% & 30.6\\% & 18.0\\% & \\hp3.8\\% \\\\[1pt] & & CNN-A-Mix & $\\epsilon_\\mathrm{train} = \\frac{8.8}{255}$ & 56.7\\% & 26.4\\% & \\hp9.0\\% & \\hp0.1\\% \\\\[1pt] \\hline \\end{tabular} } \\end{footnotesize} \\end{center} \\caption{Comparison of verified accuracy across various networks and perturbation radii. All \\sdpfo numbers computed on the first 100 test set examples, and numbers for \\textbf{LP} on the first 1000 test set examples. The perturbations and training-modes considered here differ from those in Table \\ref{tab:results_summary}. For all networks, \\sdpfo outperforms the \\textbf{LP}-relaxation baseline. }\\label{tab:extra_results} \\vspace{-5pt} \\end{table}",
    "\\begin{tabular}{lll|cc|cccc} \\hline & & & \\multicolumn{2}{c|}{Accuracy} & \\multicolumn{4}{c}{Verified Accuracy} \\\\ {\\bf Dataset} & {\\bf Epsilon} & {\\bf Model} & {\\bf Nominal} & {\\bf PGD} & {\\bf SDP-FO (Ours)} & {\\bf \\sdpip$^\\dagger$} & {\\bf LP} & {\\bf MIP$^\\dagger$} \\\\ \\hline \\multirow{6}{*}{\\mnist} & \\multirow{5}{*}{$\\epsilon = 0.1$} & MLP-SDP\\ccr& 97.6\\% & 86.4\\% & {\\bf\\hd85.2\\%} & 80\\% & 39.5\\% & 69.2\\% \\\\ & & MLP-LP \\ccr& 92.8\\% & 81.2\\% & {\\bf\\hd80.2\\%} & 80\\% & 79.4\\% & - \\\\ & & \\CH MLP-Adv\\ccr& \\CH 98.4\\% & \\CH93.4\\% & {\\bf\\CH\\hd91.0}\\% &\\CH 82\\% &\\CH\\ 26.6\\% & \\CH - \\\\ & & \\CH MLP-Adv-B\\ccs& \\CH 96.8\\% & \\CH84.0\\% & {\\bf \\CH79.2\\%} &\\CH - &\\CH\\ 33.2\\% & \\CH 34.4\\%\\\\ & & \\CH CNN-A-Adv & \\CH 99.1\\% & \\CH 95.2\\%& {\\bf \\CH87.8\\%} &\\CH - &\\CH\\hp0.4\\% & \\CH - \\\\\\cline{2-2} & \\multirow{1}{*}{$\\epsilon = 0.05$} & \\CH MLP-Nor \\ccs& \\CH 98.0\\% & \\CH46.6\\% & {\\bf \\CH28.0\\%} &\\CH - &\\CH\\ \\hp1.8\\% & \\CH \\hp6.0\\% \\\\ \\hline \\multirow{4}{*}{\\cifar} & \\multirow{4}{*}{$\\epsilon = \\frac{2}{255}$} & CNN-A-Mix-4 & 67.8\\% & 55.6\\% & {\\bf 47.8\\%} & $^{*}$ & 26.8\\% & - \\\\ & & CNN-B-Adv-4 & 72.0\\% & 62.0\\% & {\\bf 46.0\\%} & $^{*}$ & 20.4\\% & - \\\\ & &\\CH CNN-A-Mix& \\CH74.2\\% & \\CH53.0\\% & {\\bf \\CH39.6\\%} & \\CH$^{*}$ & \\CH\\hp5.8\\% & \\CH- \\\\ & &\\CH CNN-B-Adv& \\CH80.3\\% & \\CH64.0\\% & {\\bf \\CH32.8\\%} & \\CH$^{*}$ & \\CH\\hp2.2\\% & \\CH- \\\\ \\hline \\end{tabular}",
    "\\begin{table}[t] \\begin{minipage}[c]{0.48\\textwidth}% \\begin{center} \\begin{footnotesize} \\setlength\\tabcolsep{1.5pt} \\renewcommand{\\arraystretch}{1.1} \\begin{tabular}{l|cccc} \\hline {\\bf Model} & {\\bf PGD} & {\\bf SDP-FO}& {\\bf LP} & {\\bf SDP-IP} \\\\ & {\\bf (lower bound)} & & & \\\\ \\hline Grad-NN & 14.43\\% & \\textbf{16.32\\%} & 97\\% & 18\\%\\footnotemark[2]\\\\ LP-NN & 18.73\\% & \\textbf{18.97\\%} & 22\\% & 20\\%\\footnotemark[2]\\\\ PGD-NN & 6.96\\% & \\textbf{7.77\\%} & 72.1\\% & \\textbf{20\\%}\\footnotemark[2]\\\\ \\hline \\end{tabular} \\end{footnotesize} \\end{center} \\caption{Verified error rate as computed by our method SDP-FO compared to the LP approach from \\citep{kolter2017provable} and the SDP-IP approach from \\citep{raghunathan2018semidefinite}. Network architectures are described in \\citep{raghunathan2018semidefinite}. \\label{table:small-nn}} \\end{minipage} \\qquad \\begin{minipage}[c]{0.51\\textwidth}% \\centering \\includegraphics[width=.95\\textwidth]{aditi.pdf} \\vspace{.1cm} \\caption{Fraction of unverified samples as solution time increases.} \\label{fig:time-small-nn} \\end{minipage} \\end{table}",
    "\\begin{table}[h!] \\begin{center} \\begin{footnotesize} \\begin{tabular}{cc} \\hline {\\bf Encoder} & {\\bf Decoder} \\\\ \\hline \\textsc{FC 512} & \\textsc{FC 1568} \\\\ \\textsc{FC 512} & \\textsc{ CONV-T 32 3×3+2} \\\\ \\textsc{FC 512} & \\textsc{ CONV-T 3×3+1} \\\\ \\textsc{FC 16} & \\\\ \\hline \\end{tabular} \\end{footnotesize} \\end{center} \\caption{ The VAE consists of an encoder and a decoder, and the architecture details for both the encoder and the decoder are provided here. \\textsc{CONV-T t w×h+s} corresponds to a transpose convolutional layer with \\textsc{t} filters of size \\textsc{w×h} with stride of \\textsc{s} in both dimensions. }\\label{tab:vae_arch} \\end{table}"
]